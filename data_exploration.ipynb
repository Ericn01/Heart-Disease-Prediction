{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646cc8c2",
   "metadata": {},
   "source": [
    "# Data Exploration & Cleaning Notebook\n",
    "\n",
    "**Project**: Heart Disease Prediction\n",
    "\n",
    "**Date**: 2026-01-20\n",
    "\n",
    "The goals of this analysis are:\n",
    "\n",
    "1) Perform a thorough analysis of the \n",
    "\n",
    "Column Descriptions:\n",
    "\n",
    "| Var. Name |  Role | Type | Units | Notes |\n",
    "|------|------|------|-------|-------|\n",
    "|  Age |  Predictor  |  Int  | years | Age of the patient |\n",
    "|  Sex |  Predictor  |  Categorical | N/A | Male/Female  |\n",
    "| Chest Pain | Predictor | Categorical | N/A | Chest pain type |\n",
    "| Rest BP | Predictor | Integer | mmHg | Resting blood pressure (on admission to the hospital) |\n",
    "| Chol | Predictor | Integer | mg/dl | Serum cholesterol |\n",
    "| FBS | Predictor | Categorical | N/A | Fasting blood sugar > 120 mg/dl |\n",
    "| Rest ECG | Predictor | Categorical | N/A | Resting electrocardiographic results |\n",
    "| Max HR | Predictor | Integer | bpm | \tMaximum heart rate achieved |\n",
    "| Ex Angina | Predictor | Categorical | N/A | exercise-induced angina |\n",
    "| Oldpeak | Predictor | Integer |  mm | ST depression induced by exercise relative to rest |\n",
    "| Slope | Predictor | Categorical | N/A | The slope of the peak exercise ST segment |\n",
    "| Ca | Predictor |  Integer | N/A | Number of major vessels (0-3) colored by fluoroscopy |\n",
    "| Thal | Predictor | Categorical | N/A | normal; fixed defect; reversible defect | \n",
    "| CVD Class | Target | Integer | N/A | Diagnosis of heart disease (0-4) |\n",
    "\n",
    "Additional Information:\n",
    "\n",
    "Chest Pain Type: \n",
    " typical angina, atypical angina, non-anginal, asymptomatic\n",
    "\n",
    "Rest ECG: \n",
    "normal, stt abnormality, lv hypertrophy\n",
    "\n",
    "Thal: \n",
    "normal; fixed defect; reversible defect\n",
    "\n",
    "CVD Class\n",
    "0=no heart disease; 1,2,3,4 = stages of heart disease \n",
    "\n",
    "Creators:\n",
    "\n",
    "Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc724a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "files = ['cleveland.data', 'hungarian.data', 'switzerland.data', 'va.data']\n",
    "directory, file_prefix = 'data', 'processed'\n",
    "\n",
    "# Anynomous function to create the relative file paths \n",
    "make_filepath = lambda directory, file_prefix, filename: f\"./{directory}/{file_prefix}.{filename}\"\n",
    "filepaths = [make_filepath(directory, file_prefix, f) for f in files]\n",
    "\n",
    "# Defining the column names \n",
    "cols = [\"Age\", \"Sex\", \"Chest Pain\", \"Rest BP\", \"Chol\", \"FBS\", \"Rest ECG\", \"Max HR\", \"Ex Angina\", \"Oldpeak\", \"Slope\", \"Ca\", \"Thal\", \"CVD Class\"]\n",
    "dataset_names = ['Cleveland', 'Hungarian', 'Switzerland', 'VA Long Beach']\n",
    "\n",
    "dfs = [pd.read_csv(f, sep=\",\") for f in filepaths]\n",
    "\n",
    "# Assign the same column name to each dataframe\n",
    "for df in dfs:\n",
    "    df.columns = cols\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df['Dataset'] = dataset_names[i]\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]:,}, Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d20dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nDetailed data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Basic info\n",
    "print(\"\\n=== Dataset Info ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "df[numeric_cols].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "unique_count = df[categorical_cols].nunique()\n",
    "\n",
    "unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all the question marks with NaN values\n",
    "df = df.replace(\"?\", np.nan)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674606f",
   "metadata": {},
   "source": [
    "Here we face a dillema with the missing values. I'll break them down into two categories: high missingness (>30% missing) and low missingness (<10% missing).\n",
    "\n",
    "I want to use the best approach possible for dealing with these features. For example, might it be better to perform imputation on the columns that have low missingness (<10% of total rows)? For those that have a significant number of missing values (Ca, Thal, Slope), perhaps performing some kind of unsupervised learning method to approximate the most likely value using other features would make sense? Perhaps simply dropping the features would be sufficient.\n",
    "\n",
    "When looking at the description of the high missingness features, we see that they are all cardiac stress test results that require specialized equipment or procedures, which explains why a large quantity are missing. Because they are medically significant indicators of cardiac health, simply dropping these predictors will likely not improve the model's performance.\n",
    "\n",
    "- **ca**: number of major vessels (0-3) colored by flourosopy\n",
    "\n",
    "- **thal**: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "- **slope**: the slope of the peak exercise ST segment \n",
    "    -- Value 1: upsloping \n",
    "    -- Value 2: flat \n",
    "    -- Value 3: downsloping\n",
    "\n",
    "For the other values, using simple imputation is likely to be fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22391205",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = missing_data.index \n",
    "\n",
    "# Create binary missingness indicators\n",
    "missing_matrix = df[missing_cols].isnull().astype(int)\n",
    "missing_matrix.columns = [f\"{col}_missing\" for col in missing_cols]\n",
    "\n",
    "# Calculate co-occurrence (correlation of missingness)\n",
    "cooccurrence = missing_matrix.corr()\n",
    "\n",
    "cooccurrence.head(3).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation of missing features\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "mask = np.triu(np.ones_like(cooccurrence, dtype=bool), k=1)\n",
    "sns.heatmap(cooccurrence, annot=True, fmt='.2f', cmap='vlag',\n",
    "            center=0, square=True, mask=mask, cbar_kws={\"shrink\": 0.8}, ax=ax[0])\n",
    "ax[0].set_title('Missingness Co-occurrence Matrix', )\n",
    "\n",
    "# Cluster features by missingness patterns\n",
    "linkage_matrix = linkage(missing_matrix.T, method='ward')\n",
    "\n",
    "dendrogram(linkage_matrix, labels=[col.replace('_missing', '') \n",
    "                                    for col in missing_matrix.columns],\n",
    "           leaf_rotation=50, leaf_font_size=10, ax=ax[1])\n",
    "ax[1].set_title('Hierarchical Clustering of Features by Missingness Patterns')\n",
    "ax[1].set_xlabel('Features')\n",
    "ax[1].set_ylabel('Distance')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf329c",
   "metadata": {},
   "source": [
    "Looking at the results from this analysis, it is clear that the patterns in missingness are not a coincidence. The dendogram and correlation matrix confirm that Slope, Ca, and Thal have a high co-occurence rate and are clustered together. Nonetheless, other features that have lower missing incidence seem to have even greater correlation and smaller distances from each other than the features that are missing a significant amount of values.\n",
    "\n",
    "This may indicate that certain regions perform these tests together, and others may forgo them entirely. It's a nice proxy visualization for the four different datasets that are merged together, indicating the population differences that exist between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "high_miss_features = ['Ca', 'Thal', 'Slope']\n",
    "target_col = 'CVD Class'\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "for idx, feature in enumerate(high_miss_features):\n",
    "    if idx < len(high_miss_features):\n",
    "        is_missing = df[feature].isnull()\n",
    "        cross_tab = pd.crosstab(df[target_col], is_missing, normalize='columns') * 100\n",
    "        \n",
    "        cross_tab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
    "        axes[idx].set_title(f'{feature} Missingness vs {target_col}')\n",
    "        axes[idx].set_xlabel(target_col)\n",
    "        axes[idx].set_ylabel('Percentage')\n",
    "        axes[idx].legend(['Present', 'Missing'], loc='best')\n",
    "        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45)\n",
    "        \n",
    "        # Chi-square test\n",
    "        contingency = pd.crosstab(df[target_col], is_missing)\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "        axes[idx].text(0.02, 0.98, f'p-value: {p_value:.3f}', \n",
    "                        transform=axes[idx].transAxes, \n",
    "                        verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='skyblue', alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d47fe6",
   "metadata": {},
   "source": [
    "The results above suggest that the individuals who have missing values and those who have the values present across the top three missing predictors (Ca, Thal, Slope), come from a different population. This makes sense as the dataset is composed of individuals from different regions of the world (United States, Hungary, Switzerland) which may have very different lifestyles and diets, along with wealth (which may explain why )\n",
    "\n",
    "As a result of this analysis we can confidently say there is a pattern that exists within the missing data, and therefore it is *Missing Not At Random (MNAR)*.\n",
    "\n",
    "## Implications\n",
    "\n",
    "- Missingness is structured by data source (different hospitals/regions).\n",
    "- The populations are fundamentally different (lifestyle, diet, wealth, healtcare system).\n",
    "- Missing tests likely reflect different diagnostic protocols rather than random omission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11810319",
   "metadata": {},
   "source": [
    "## Verdict\n",
    "\n",
    "Perhaps imputation should occur on a dataset-by-dataset basis. Since each dataset comes from a different population, an analysis of the different datasets; observing and quantifying their similarities and differences would be valuable. The end goal for this project is to build a model that is good at predicting what category of cardiovascular disease someone belongs to (0-4), where 0=no heart disease; 1,2,3,4 = stages of heart disease.\n",
    "\n",
    "The hypothesis is that there are common factors regardless of the population we're studying that will influence the development, and as a result, the stage of heart disease in an individual. In this part of the project however, I want to quantify and explore the differences between the different datasets to gain a deeper understanding of the different populations.\n",
    "\n",
    "This will be done in the next section below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984c3c2",
   "metadata": {},
   "source": [
    "# Dataset (Population) Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_stats = []\n",
    "for name, df in zip(dataset_names, dfs):\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    df.replace('?', np.nan, inplace=True) # Important step as they won't be count as missing if this isn't done \n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    complete_cases = (~df.isnull().any(axis=1)).sum()\n",
    "    \n",
    "    overview_stats.append({\n",
    "        'Dataset': name,\n",
    "        'N_Observations': df.shape[0],\n",
    "        'N_Features': df.shape[1],\n",
    "        'Complete_Cases': complete_cases,\n",
    "        'Complete_Cases_%': (complete_cases / df.shape[0] * 100),\n",
    "        'Missing_Cells': missing_cells,\n",
    "        'Missing_%': (missing_cells / total_cells * 100)\n",
    "    })\n",
    "\n",
    "overview_df = pd.DataFrame(overview_stats)\n",
    "\n",
    "overview_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08042f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset sizes and completeness\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sample sizes\n",
    "axes[0].bar(dataset_names, [df.shape[0] for df in dfs], \n",
    "            color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "axes[0].set_title('Sample Size by Dataset')\n",
    "axes[0].set_ylabel('Number of Observations')\n",
    "axes[0].set_xlabel('Dataset')\n",
    "for i, (name, df) in enumerate(zip(dataset_names, dfs)):\n",
    "    axes[0].text(i, df.shape[0] + 5, str(df.shape[0]), \n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "# Completeness\n",
    "completeness = [overview_stats[i]['Complete_Cases_%'] for i in range(len(dfs))]\n",
    "axes[1].bar(dataset_names, completeness, \n",
    "            color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "axes[1].set_title('Data Completeness by Dataset')\n",
    "axes[1].set_ylabel('% Complete Cases')\n",
    "axes[1].set_xlabel('Dataset')\n",
    "for i, val in enumerate(completeness):\n",
    "    axes[1].text(i, val + 1, f'{val:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missingness percentage for each feature in each dataset\n",
    "missingness_comparison = pd.DataFrame()\n",
    "for name, df in zip(dataset_names, dfs):\n",
    "    miss_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    missingness_comparison[name] = miss_pct\n",
    "\n",
    "missingness_comparison = missingness_comparison.drop('Dataset', errors='ignore')\n",
    "missingness_comparison = missingness_comparison[missingness_comparison.sum(axis=1) > 0]\n",
    "\n",
    "print(\"\\nMissingness % by Feature and Dataset:\")\n",
    "missingness_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missingness patterns\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(14, 5))\n",
    "\n",
    "missingness_comparison.T.plot(kind='barh', ax=ax[0])\n",
    "ax[0].set_title('Feature Missingness Across Datasets')\n",
    "ax[0].set_ylabel('Dataset')\n",
    "ax[0].set_xlabel('Missing (%)')\n",
    "ax[0].legend(title='Features', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Heatmap version\n",
    "sns.heatmap(missingness_comparison, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Missing %'}, ax=ax[1])\n",
    "ax[1].set_title('Missingness Heatmap: Features vs Datasets')\n",
    "ax[1].set_xlabel('Dataset')\n",
    "ax[1].set_ylabel('Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a93bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation\n",
    "cvd_crosstab = pd.crosstab(df['Dataset'], df['CVD Class'], \n",
    "                            margins=True, margins_name='Total')\n",
    "print(\"\\nAbsolute counts:\")\n",
    "print(cvd_crosstab)\n",
    "\n",
    "# Percentage distribution (excluding margins)\n",
    "cvd_pct = pd.crosstab(df['Dataset'], df['CVD Class'], \n",
    "                      normalize='index') * 100\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(cvd_pct.round(2))\n",
    "\n",
    "# Chi-square test for independence\n",
    "contingency = pd.crosstab(df['Dataset'], df['CVD Class'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(f\"\\nChi-square test for independence:\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")\n",
    "print(f\"Degrees of freedom: {dof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the CVD class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Stacked bar chart\n",
    "cvd_pct.plot(kind='bar', stacked=True, ax=axes[0], color=['#2ecc71', '#f39c12', '#e67e22', '#e74c3c', '#9b59b6'])\n",
    "axes[0].set_title('CVD Class Distribution by Dataset (Stacked %)')\n",
    "axes[0].set_xlabel('Dataset')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].legend(title='CVD Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Grouped bar chart\n",
    "cvd_pct.plot(kind='bar', ax=axes[1], color=['#2ecc71', '#f39c12', '#e67e22', '#e74c3c', '#9b59b6'])\n",
    "axes[1].set_title('CVD Class Distribution by Dataset (Grouped %)')\n",
    "axes[1].set_xlabel('Dataset')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].legend(title='CVD Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
